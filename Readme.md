# ğŸš€ NLP Sentiment Monitoring API

A production-style NLP Sentiment Monitoring system built with FastAPI that performs real-time inference, logging, drift detection, and automated retraining simulation.

This project demonstrates how machine learning models can be deployed, monitored, and continuously improved in a production-like environment.

---

## ğŸ“Œ Key Features

* âœ… FastAPI-based inference API
* âœ… Real-time latency and confidence logging
* âœ… Rolling-window drift detection
* âœ… Automated retraining trigger (async)
* âœ… Model versioning and registration
* âœ… Payload validation and API hardening
* âœ… Production-style monitoring architecture

---

## ğŸ—ï¸ System Architecture

Client
â†’ FastAPI Inference API
â†’ Text Preprocessing
â†’ Vectorizer + ML Model
â†’ Prediction + Confidence
â†’ Metrics Logging
â†’ Drift Detection
â†’ Automated Retraining
â†’ Model Registry

(Architecture diagram included in repository.)

---

## âš™ï¸ Tech Stack

* Python
* FastAPI
* scikit-learn
* NLTK
* Joblib
* Threading
* Logging

---

## ğŸ““ Training Notebook
* Model training and preprocessing are documented in /notebooks/sentiment_training.ipynb.

---
## ğŸ“‚ Project Structure

```
.
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ schemas.py
â”‚   â””â”€â”€ logger.py
â”œâ”€â”€ model/
â”‚   â”œâ”€â”€ sentiment_model.pkl
â”‚   â””â”€â”€ vectorizer.pkl
â”œâ”€â”€ monitoring/
â”‚   â””â”€â”€ drift.py
â”œâ”€â”€ retraining/
â”‚   â””â”€â”€ retrain.py
â””â”€â”€ README.md
```

---

## â–¶ï¸ Running Locally

1. Install dependencies

```bash
pip install -r requirements.txt
```

2. Start the API

```bash
uvicorn api.main:app --reload
```

3. Open Swagger UI

```
http://localhost:8000/docs
```

---

## ğŸ§ª Example Request

```json
{
  "text": "I really love this product. It works perfectly."
}
```

---

## ğŸ“¤ Example Response

```json
{
  "sentiment": "positive",
  "confidence": 0.93,
  "drift": {
    "drift": false
  }
}
```

---

## ğŸ“Š Drift Monitoring Logic

* Tracks rolling statistics of:

  * Input text length
  * Prediction confidence
* Compares live distribution against baseline window
* Flags drift when deviation crosses threshold
* Automatically triggers retraining pipeline

---

## ğŸ” Automated Retraining

When drift is detected:

* A background thread triggers retraining
* A new model version is trained
* Model is registered and versioned
* System continues serving traffic

Concurrency protection prevents duplicate retraining jobs.

---

## ğŸ”’ Production Considerations

* JSON payload validation
* Size limits for text input
* Logging for observability
* Thread-safe retraining execution
* Modular design for scalability

---

## ğŸŒ± Future Improvements

* Prometheus metrics integration
* Model performance dashboards
* Canary deployments
* Shadow testing
* Cloud deployment
* CI/CD pipeline automation

---

## ğŸ‘¨â€ğŸ’» Author

Built as a portfolio project to demonstrate real-world MLOps and backend ML engineering skills.
